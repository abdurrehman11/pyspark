{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.5-bin-hadoop2.7'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 23|Gilbert|\n",
      "| 19|  Alexa|\n",
      "| 27|    May|\n",
      "| 31|Deloise|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----+\n",
      "|summary|              age| name|\n",
      "+-------+-----------------+-----+\n",
      "|  count|                4|    4|\n",
      "|   mean|             25.0| null|\n",
      "| stddev|5.163977794943222| null|\n",
      "|    min|               19|Alexa|\n",
      "|    max|               31|  May|\n",
      "+-------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, \n",
    "                               IntegerType, StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "               StructField('name', StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"people.json\", schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 23|Gilbert|\n",
      "| 19|  Alexa|\n",
      "| 27|    May|\n",
      "| 31|Deloise|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Row and Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 23|\n",
      "| 19|\n",
      "| 27|\n",
      "| 31|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=23, name='Gilbert'), Row(age=19, name='Alexa')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 23|Gilbert|\n",
      "| 19|  Alexa|\n",
      "| 27|    May|\n",
      "| 31|Deloise|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|age|   name|newage|\n",
      "+---+-------+------+\n",
      "| 23|Gilbert|    23|\n",
      "| 19|  Alexa|    19|\n",
      "| 27|    May|    27|\n",
      "| 31|Deloise|    31|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('newage', df['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "|age|   name|double_age|\n",
      "+---+-------+----------+\n",
      "| 23|Gilbert|        46|\n",
      "| 19|  Alexa|        38|\n",
      "| 27|    May|        54|\n",
      "| 31|Deloise|        62|\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 23|Gilbert|\n",
      "| 19|  Alexa|\n",
      "| 27|    May|\n",
      "| 31|Deloise|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|my_new_age|   name|\n",
      "+----------+-------+\n",
      "|        23|Gilbert|\n",
      "|        19|  Alexa|\n",
      "|        27|    May|\n",
      "|        31|Deloise|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age', 'my_new_age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrame to SQL View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT * FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 23|Gilbert|\n",
      "| 19|  Alexa|\n",
      "| 27|    May|\n",
      "| 31|Deloise|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = spark.sql(\"SELECT * FROM people WHERE age=31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 31|Deloise|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"AAPL.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|2019-05-13 00:00:00|187.710007|189.479996|182.850006|185.720001|183.529678|57430600|\n",
      "|2019-05-14 00:00:00|186.410004|189.699997|185.410004|188.660004|186.434998|36529700|\n",
      "|2019-05-15 00:00:00|186.270004|    191.75|186.020004|190.919998|188.668335|26544700|\n",
      "|2019-05-16 00:00:00|189.910004|192.470001|188.839996|190.080002|187.838257|33031400|\n",
      "|2019-05-17 00:00:00|186.929993|190.899994|186.759995|     189.0|186.770996|32879100|\n",
      "|2019-05-20 00:00:00|183.520004|184.350006|180.279999|183.089996|180.930695|38612300|\n",
      "|2019-05-21 00:00:00|185.220001|     188.0|184.699997|186.600006|184.399307|28364800|\n",
      "|2019-05-22 00:00:00|184.660004|185.710007|182.550003|182.779999|180.624329|29748600|\n",
      "|2019-05-23 00:00:00|179.800003|180.539993|177.809998|179.660004|177.541138|36529700|\n",
      "|2019-05-24 00:00:00|180.199997|182.139999|178.619995|178.970001|176.859283|23714700|\n",
      "|2019-05-28 00:00:00|178.919998|180.589996|177.910004|178.229996|176.128006|27948200|\n",
      "|2019-05-29 00:00:00|176.419998|179.350006|     176.0|177.380005|175.288025|28481200|\n",
      "|2019-05-30 00:00:00|177.949997|179.229996|176.669998|178.300003|176.197189|21218400|\n",
      "|2019-05-31 00:00:00|176.229996|177.990005|174.990005|175.070007| 173.00528|27043600|\n",
      "|2019-06-03 00:00:00|175.600006|177.919998|170.270004|173.300003|171.256134|40396100|\n",
      "|2019-06-04 00:00:00|175.440002|179.830002|174.520004|179.639999|177.521378|30968000|\n",
      "|2019-06-05 00:00:00|184.279999|184.990005|181.139999|182.539993|180.387146|29773400|\n",
      "|2019-06-06 00:00:00|183.080002|185.470001|182.149994|185.220001|183.035568|22526300|\n",
      "|2019-06-07 00:00:00|186.509995|191.919998|185.770004|190.149994| 187.90741|30684400|\n",
      "|2019-06-10 00:00:00|191.809998|195.369995|191.619995|192.580002|190.308762|26220900|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2019, 5, 14, 0, 0), Open=186.410004, High=189.699997, Low=185.410004, Close=188.660004, Adj Close=186.434998, Volume=36529700)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|2019-05-13 00:00:00|187.710007|189.479996|182.850006|185.720001|183.529678|57430600|\n",
      "|2019-05-14 00:00:00|186.410004|189.699997|185.410004|188.660004|186.434998|36529700|\n",
      "|2019-05-15 00:00:00|186.270004|    191.75|186.020004|190.919998|188.668335|26544700|\n",
      "|2019-05-16 00:00:00|189.910004|192.470001|188.839996|190.080002|187.838257|33031400|\n",
      "|2019-05-17 00:00:00|186.929993|190.899994|186.759995|     189.0|186.770996|32879100|\n",
      "|2019-05-20 00:00:00|183.520004|184.350006|180.279999|183.089996|180.930695|38612300|\n",
      "|2019-05-21 00:00:00|185.220001|     188.0|184.699997|186.600006|184.399307|28364800|\n",
      "|2019-05-22 00:00:00|184.660004|185.710007|182.550003|182.779999|180.624329|29748600|\n",
      "|2019-06-05 00:00:00|184.279999|184.990005|181.139999|182.539993|180.387146|29773400|\n",
      "|2019-06-06 00:00:00|183.080002|185.470001|182.149994|185.220001|183.035568|22526300|\n",
      "|2019-06-07 00:00:00|186.509995|191.919998|185.770004|190.149994| 187.90741|30684400|\n",
      "|2019-06-10 00:00:00|191.809998|195.369995|191.619995|192.580002|190.308762|26220900|\n",
      "|2019-06-11 00:00:00|194.860001|     196.0|193.600006|194.809998|192.512451|26932900|\n",
      "|2019-06-12 00:00:00|193.949997|195.970001|193.389999|194.190002| 191.89978|18221800|\n",
      "|2019-06-13 00:00:00|194.699997|196.789993|193.600006|194.149994|191.860229|21674600|\n",
      "|2019-06-14 00:00:00|191.550003|193.589996|190.300003|192.740005|190.466873|18761500|\n",
      "|2019-06-17 00:00:00|192.899994|194.960007|192.169998|193.889999|191.603317|14669100|\n",
      "|2019-06-18 00:00:00|196.050003|200.289993|195.210007|198.449997|196.109528|26551000|\n",
      "|2019-06-19 00:00:00|199.679993|199.880005|197.309998|197.869995|195.536377|21124200|\n",
      "|2019-06-20 00:00:00|200.369995|200.610001|198.029999|199.460007| 197.10762|21514000|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close > 180').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|187.710007|185.720001|\n",
      "|186.410004|188.660004|\n",
      "|186.270004|190.919998|\n",
      "|189.910004|190.080002|\n",
      "|186.929993|     189.0|\n",
      "|183.520004|183.089996|\n",
      "|185.220001|186.600006|\n",
      "|184.660004|182.779999|\n",
      "|184.279999|182.539993|\n",
      "|183.080002|185.220001|\n",
      "|186.509995|190.149994|\n",
      "|191.809998|192.580002|\n",
      "|194.860001|194.809998|\n",
      "|193.949997|194.190002|\n",
      "|194.699997|194.149994|\n",
      "|191.550003|192.740005|\n",
      "|192.899994|193.889999|\n",
      "|196.050003|198.449997|\n",
      "|199.679993|197.869995|\n",
      "|200.369995|199.460007|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close > 180').select(['Open', 'Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Open=187.710007, Close=185.720001),\n",
       " Row(Open=186.410004, Close=188.660004),\n",
       " Row(Open=186.270004, Close=190.919998),\n",
       " Row(Open=189.910004, Close=190.080002),\n",
       " Row(Open=186.929993, Close=189.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter('Close > 180').select(['Open', 'Close']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  Volume|\n",
      "+--------+\n",
      "|57430600|\n",
      "|36529700|\n",
      "|26544700|\n",
      "|33031400|\n",
      "|32879100|\n",
      "|38612300|\n",
      "|28364800|\n",
      "|29748600|\n",
      "|36529700|\n",
      "|23714700|\n",
      "|27948200|\n",
      "|28481200|\n",
      "|21218400|\n",
      "|27043600|\n",
      "|40396100|\n",
      "|30968000|\n",
      "|29773400|\n",
      "|22526300|\n",
      "|30684400|\n",
      "|26220900|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close'] < 500).select(['Volume']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|2019-06-20 00:00:00|200.369995|200.610001|198.029999|199.460007| 197.10762|21514000|\n",
      "|2019-06-27 00:00:00|200.289993|201.570007|199.570007|199.740005|197.384323|20899700|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 200) & (df['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|2019-05-13 00:00:00|187.710007|189.479996|182.850006|185.720001|183.529678|57430600|\n",
      "|2019-05-14 00:00:00|186.410004|189.699997|185.410004|188.660004|186.434998|36529700|\n",
      "|2019-05-15 00:00:00|186.270004|    191.75|186.020004|190.919998|188.668335|26544700|\n",
      "|2019-05-16 00:00:00|189.910004|192.470001|188.839996|190.080002|187.838257|33031400|\n",
      "|2019-05-17 00:00:00|186.929993|190.899994|186.759995|     189.0|186.770996|32879100|\n",
      "|2019-05-20 00:00:00|183.520004|184.350006|180.279999|183.089996|180.930695|38612300|\n",
      "|2019-05-21 00:00:00|185.220001|     188.0|184.699997|186.600006|184.399307|28364800|\n",
      "|2019-05-22 00:00:00|184.660004|185.710007|182.550003|182.779999|180.624329|29748600|\n",
      "|2019-05-23 00:00:00|179.800003|180.539993|177.809998|179.660004|177.541138|36529700|\n",
      "|2019-05-24 00:00:00|180.199997|182.139999|178.619995|178.970001|176.859283|23714700|\n",
      "|2019-05-28 00:00:00|178.919998|180.589996|177.910004|178.229996|176.128006|27948200|\n",
      "|2019-05-29 00:00:00|176.419998|179.350006|     176.0|177.380005|175.288025|28481200|\n",
      "|2019-05-30 00:00:00|177.949997|179.229996|176.669998|178.300003|176.197189|21218400|\n",
      "|2019-05-31 00:00:00|176.229996|177.990005|174.990005|175.070007| 173.00528|27043600|\n",
      "|2019-06-03 00:00:00|175.600006|177.919998|170.270004|173.300003|171.256134|40396100|\n",
      "|2019-06-04 00:00:00|175.440002|179.830002|174.520004|179.639999|177.521378|30968000|\n",
      "|2019-06-05 00:00:00|184.279999|184.990005|181.139999|182.539993|180.387146|29773400|\n",
      "|2019-06-06 00:00:00|183.080002|185.470001|182.149994|185.220001|183.035568|22526300|\n",
      "|2019-06-07 00:00:00|186.509995|191.919998|185.770004|190.149994| 187.90741|30684400|\n",
      "|2019-06-10 00:00:00|191.809998|195.369995|191.619995|192.580002|190.308762|26220900|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close'] < 200) & ~(df['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "|2019-05-30 00:00:00|177.949997|179.229996|176.669998|178.300003|176.197189|21218400|\n",
      "+-------------------+----------+----------+----------+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Low'] == 176.669998).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter(df['Low'] == 176.669998).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2019, 5, 30, 0, 0), Open=177.949997, High=179.229996, Low=176.669998, Close=178.300003, Adj Close=176.197189, Volume=21218400)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2019, 5, 30, 0, 0),\n",
       " 'Open': 177.949997,\n",
       " 'High': 179.229996,\n",
       " 'Low': 176.669998,\n",
       " 'Close': 178.300003,\n",
       " 'Adj Close': 176.197189,\n",
       " 'Volume': 21218400}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21218400"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sales_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1dd4c896508>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Company').mean().show()\n",
    "# df.groupBy('Company').max().show()\n",
    "# df.groupBy('Company').min().show()\n",
    "# df.groupBy('Company').count().show()\n",
    "# df.groupBy('Company').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Sales': 'sum'}).show()\n",
    "# df.agg({'Sales': 'min'}).show()\n",
    "# df.agg({'Sales': 'max'}).show()\n",
    "# df.agg({'Sales': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = df.groupBy('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data.agg({'Sales': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n",
      "+-----------+\n",
      "|Sales count|\n",
      "+-----------+\n",
      "|         11|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Sales')).show()\n",
    "# df.select(avg('Sales')).show()\n",
    "# df.select(stddev('Sales')).show()\n",
    "\n",
    "df.select(countDistinct('Sales').alias('Sales count')).show()\n",
    "# df.select(avg('Sales').alias('Sales Avg.')).show()\n",
    "# df.select(stddev('Sales').alias('Sales STD')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std = df.select(stddev('Sales').alias('std'))\n",
    "sales_std.select(format_number('std', 2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Sales').show()\n",
    "df.orderBy(df['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('ContainsNull.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop only those rows which have atleast 2 null\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by default -> drop has 'any'\n",
    "df.na.drop(how='any').show()\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| null|\n",
      "|emp2|FILL VALUE| null|\n",
      "|emp3|FILL VALUE|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "+----+----------+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('FILL VALUE').show()\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = df.select(mean(df['Sales'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.5\n"
     ]
    }
   ],
   "source": [
    "mean_sales = mean_val[0][0]\n",
    "print(mean_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_sales, subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# short form for filling values\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0], subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('AAPL.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2019, 5, 13, 0, 0), Open=187.710007, High=189.479996, Low=182.850006, Close=185.720001, Adj Close=183.529678, Volume=57430600)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(['Date', 'Open']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth, hour,\n",
    "                                   dayofyear, month,\n",
    "                                   year, weekofyear,\n",
    "                                   format_number, date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(dayofmonth(df['Date'])).show()\n",
    "# df.select(hour(df['Date'])).show()\n",
    "# df.select(dayofyear(df['Date'])).show()\n",
    "# df.select(month(df['Date'])).show()\n",
    "# df.select(year(df['Date'])).show()\n",
    "# df.select(weekofyear(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.withColumn('Year', year(df['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_df.groupBy('Year').mean().select(['Year', 'avg(Close)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = result.withColumnRenamed('avg(Close)', 'Avergae Closing Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|Avg Close|\n",
      "+----+---------+\n",
      "|2019|   223.93|\n",
      "|2020|   289.62|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_result.select(['Year', format_number('Avergae Closing Price', 2).alias('Avg Close')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
